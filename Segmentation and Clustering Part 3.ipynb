{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "import csv\n",
    "import os\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "from bs4 import BeautifulSoup  #### Scraping library\n",
    "\n",
    "print('Libraries imported.')\n",
    "\n",
    "website_url = \"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\"\n",
    "\n",
    "### Scraping Code\n",
    "\n",
    "def scrape(url, output_name):\n",
    "    \"\"\"Create CSVs from all tables in a Wikipedia article.\n",
    "    ARGS:\n",
    "        url (str): The full URL of the Wikipedia article to scrape tables from.\n",
    "        output_name (str): The base file name (without filepath) to write to.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read tables from Wikipedia article into list of HTML strings\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "    table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "    wikitables = soup.findAll(\"table\", table_classes)\n",
    "\n",
    "    # Create folder for output if it doesn't exist\n",
    "    os.makedirs(output_name, exist_ok=True)\n",
    "\n",
    "    for index, table in enumerate(wikitables):\n",
    "        # Make a unique file name for each CSV\n",
    "        if index == 0:\n",
    "            filename = output_name\n",
    "        else:\n",
    "            filename = output_name + \"_\" + str(index)\n",
    "\n",
    "        filepath = os.path.join(output_name, filename) + \".csv\"\n",
    "\n",
    "        with open(\"canada.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as output:\n",
    "            csv_writer = csv.writer(output, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
    "            write_html_table_to_csv(table, csv_writer)\n",
    "\n",
    "\n",
    "def write_html_table_to_csv(table, writer):\n",
    "    \"\"\"Write HTML table from Wikipedia to a CSV file.\n",
    "    ARGS:\n",
    "        table (bs4.Tag): The bs4 Tag object being analyzed.\n",
    "        writer (csv.writer): The csv Writer object creating the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hold elements that span multiple rows in a list of\n",
    "    # dictionaries that track 'rows_left' and 'value'\n",
    "    saved_rowspans = []\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll([\"th\", \"td\"])\n",
    "\n",
    "        # If the first row, use it to define width of table\n",
    "        if len(saved_rowspans) == 0:\n",
    "            saved_rowspans = [None for _ in cells]\n",
    "        # Insert values from cells that span into this row\n",
    "        elif len(cells) != len(saved_rowspans):\n",
    "            for index, rowspan_data in enumerate(saved_rowspans):\n",
    "                if rowspan_data is not None:\n",
    "                    # Insert the data from previous row; decrement rows left\n",
    "                    value = rowspan_data[\"value\"]\n",
    "                    cells.insert(index, value)\n",
    "\n",
    "                    if saved_rowspans[index][\"rows_left\"] == 1:\n",
    "                        saved_rowspans[index] = None\n",
    "                    else:\n",
    "                        saved_rowspans[index][\"rows_left\"] -= 1\n",
    "\n",
    "        # If an element with rowspan, save it for future cells\n",
    "        for index, cell in enumerate(cells):\n",
    "            if cell.has_attr(\"rowspan\"):\n",
    "                rowspan_data = {\"rows_left\": int(cell[\"rowspan\"]), \"value\": cell}\n",
    "                saved_rowspans[index] = rowspan_data\n",
    "\n",
    "        if cells:\n",
    "            # Clean the data of references and unusual whitespace\n",
    "            cleaned = clean_data(cells)\n",
    "\n",
    "            # Fill the row with empty columns if some are missing\n",
    "            # (Some HTML tables leave final empty cells without a <td> tag)\n",
    "            columns_missing = len(saved_rowspans) - len(cleaned)\n",
    "            if columns_missing:\n",
    "                cleaned += [None] * columns_missing\n",
    "\n",
    "            writer.writerow(cleaned)\n",
    "            \n",
    "def clean_data(row):\n",
    "    \"\"\"Clean table row list from Wikipedia into a string for CSV.\n",
    "    ARGS:\n",
    "        row (bs4.ResultSet): The bs4 result set being cleaned for output.\n",
    "    RETURNS:\n",
    "        cleaned_cells (list[str]): List of cleaned text items in this row.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_cells = []\n",
    "\n",
    "    for cell in row:\n",
    "        # Strip references from the cell\n",
    "        references = cell.findAll(\"sup\", {\"class\": \"reference\"})\n",
    "        if references:\n",
    "            for ref in references:\n",
    "                ref.extract()\n",
    "\n",
    "        # Strip sortkeys from the cell\n",
    "        sortkeys = cell.findAll(\"span\", {\"class\": \"sortkey\"})\n",
    "        if sortkeys:\n",
    "            for ref in sortkeys:\n",
    "                ref.extract()\n",
    "\n",
    "        # Strip footnotes from text and join into a single string\n",
    "        text_items = cell.findAll(text=True)\n",
    "        no_footnotes = [text for text in text_items if text[0] != \"[\"]\n",
    "\n",
    "        cleaned = (\n",
    "            \"\".join(no_footnotes)  # Combine elements into single string\n",
    "            .replace(\"\\xa0\", \" \")  # Replace non-breaking spaces\n",
    "            .replace(\"\\n\", \" \")  # Replace newlines\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "        cleaned_cells += [cleaned]\n",
    "\n",
    "    return cleaned_cells\n",
    "\n",
    "### Save the results in 'canada.csv'\n",
    "\n",
    "scrape(website_url,'canada')\n",
    "\n",
    "### Read Data From CSV\n",
    "\n",
    "data=pd.read_csv(\"canada.csv\",engine='python',encoding='utf-8')\n",
    "data.head(5)\n",
    "\n",
    "### Remove rows where Borough is not assigned\n",
    "\n",
    "data=data[data.Borough!='Not assigned']\n",
    "data.head(5)\n",
    "\n",
    "### Assign Neighbourhood where it is not assigned\n",
    "\n",
    "data['Neighbourhood']=np.where(data.Neighbourhood=='Not assigned',data.Borough,data.Neighbourhood)\n",
    "data.head(5)\n",
    "\n",
    "### Combine Neighborhood as comma separated\n",
    "\n",
    "data=data.groupby(['Postcode','Borough']).Neighbourhood.agg([('Neighbourhood', ', '.join)]).reset_index()\n",
    "data.head(5)\n",
    "\n",
    "### Print Shape\n",
    "\n",
    "data.shape\n",
    "\n",
    "### Install and import geocoder library\n",
    "\n",
    "# !pip install geocoder\n",
    "import geocoder # import geocoder\n",
    "\n",
    "### Attempted to use Geocoder but didnt work properly due to limited attempts\n",
    "\n",
    "def getlatlong(postal_code):\n",
    "    # initialize your variable to None\n",
    "    lat_lng_coords = None\n",
    "\n",
    "    # loop until you get the coordinates\n",
    "    while(lat_lng_coords is None):\n",
    "      g = geocoder.google('{}, Toronto, Ontario'.format(postal_code))\n",
    "      lat_lng_coords = g.latlng\n",
    "\n",
    "    latitude = lat_lng_coords[0]\n",
    "    longitude = lat_lng_coords[1]\n",
    "    return latitude,longitude\n",
    "\n",
    "# data['Latitude'],data['Longitude']=data.Postcode.apply(getlatlong)\n",
    "\n",
    "### Read Lat,Long Data from CSV url given\n",
    "\n",
    "latlongdata=pd.read_csv('http://cocl.us/Geospatial_data')\n",
    "latlongdata.head(5)\n",
    "\n",
    "### Join to original data to create updated dataset\n",
    "\n",
    "data_updated=pd.merge(data,latlongdata,left_on='Postcode',right_on='Postal Code',how='left').drop('Postal Code',axis=1)\n",
    "data_updated.head(5)\n",
    "\n",
    "### Boroughs with Toronto in name\n",
    "\n",
    "neighborhoods=data_updated[data_updated.Borough.str.contains('Toronto')]\n",
    "\n",
    "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n",
    "        len(neighborhoods['Borough'].unique()),\n",
    "        neighborhoods.shape[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "address = 'Toronto, ON'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"ny_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Toronto are {}, {}.'.format(latitude, longitude))\n",
    "\n",
    "### Create map of Toronto using latitude and longitude values\n",
    "\n",
    "map_toronto = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "### Add markers to map\n",
    "\n",
    "for lat, lng, borough, neighborhood in zip(neighborhoods['Latitude'], neighborhoods['Longitude'], neighborhoods['Borough'], neighborhoods['Neighbourhood']):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_toronto)  \n",
    "    \n",
    "map_toronto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
